{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4030697c",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Data Merging & Cleaning â€“ Yousra Descriptive Stats Notebook\n",
    "\n",
    "This notebook loads the original project datasets (sales, weather, kiwo event),  \n",
    "cleans and standardizes the date format, merges them using a full outer join,  \n",
    "and performs descriptive statistics, missing value inspection, and prepares the  \n",
    "data for further feature engineering and modeling.\n",
    "\n",
    "The goal is to:\n",
    "1. Combine **all available data** on matching dates  \n",
    "2. Perform a clear descriptive exploration  \n",
    "3. Detect and handle missing values  \n",
    "4. Produce a clean dataset ready for analysis  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data from your project folder\n",
    "df_kiwo = pd.read_csv(\"data/kiwo.csv\")\n",
    "df_weather = pd.read_csv(\"data/wetter.csv\")\n",
    "df_umsatz = pd.read_csv(\"data/umsatzdaten_gekuerzt.csv\")\n",
    "\n",
    "df_kiwo.head(), df_weather.head(), df_umsatz.head()\n",
    "#print number of rows for each dataframe\n",
    "print(f\"Kiwo Data Rows: {len(df_kiwo)}\")\n",
    "print(f\"Weather Data Rows: {len(df_weather)}\")\n",
    "print(f\"Umsatz Data Rows: {len(df_umsatz)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Datum' to proper datetime format and drop invalid rows\n",
    "for df in (df_kiwo, df_weather, df_umsatz):\n",
    "    df[\"Datum\"] = pd.to_datetime(df[\"Datum\"], errors=\"coerce\").dt.normalize()\n",
    "    df.dropna(subset=[\"Datum\"], inplace=True)\n",
    "\n",
    "    # Drop duplicate date columns if exist\n",
    "    if \"date\" in df.columns:\n",
    "        df.drop(columns=[\"date\"], inplace=True)\n",
    "\n",
    "df_kiwo.info(), df_weather.info(), df_umsatz.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding suffixes helps avoid confusion during merging\n",
    "\n",
    "df_kiwo = df_kiwo.add_suffix(\"_kiwo\")\n",
    "df_kiwo.rename(columns={\"Datum_kiwo\": \"Datum\"}, inplace=True)\n",
    "\n",
    "df_weather = df_weather.add_suffix(\"_weather\")\n",
    "df_weather.rename(columns={\"Datum_weather\": \"Datum\"}, inplace=True)\n",
    "\n",
    "\n",
    "df_umsatz = df_umsatz.add_suffix(\"_umsatz\")\n",
    "df_umsatz.rename(columns={\"Datum_umsatz\": \"Datum\"}, inplace=True)\n",
    "\n",
    "df_umsatz[\"umsatz_rolling7\"] = (\n",
    "    df_umsatz[\"Umsatz_umsatz\"].rolling(window=7, min_periods=1).mean()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = (\n",
    "    df_kiwo\n",
    "    .merge(df_weather, on=\"Datum\", how=\"outer\")\n",
    "    .merge(df_umsatz, on=\"Datum\", how=\"outer\")\n",
    ")\n",
    "\n",
    "merged_df.shape\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f05e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric summary\n",
    "merged_df.describe()\n",
    "\n",
    "# For all columns (including categorical)\n",
    "merged_df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = merged_df.isna().sum()\n",
    "missing_percent = (merged_df.isna().sum() / len(merged_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    \"Missing Count\": missing_count,\n",
    "    \"Missing %\": missing_percent\n",
    "})\n",
    "\n",
    "missing_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471abb4d",
   "metadata": {},
   "source": [
    "### âœ”ï¸ Missing Value Strategy\n",
    "\n",
    "**Umsatz (Sales):**\n",
    "- Missing values mean the day has **no sales** or is **outside the bakery event period**.\n",
    "- These rows should be **removed**, not imputed.\n",
    "\n",
    "**Weather Variables:**\n",
    "- Weather data may be missing because not all dates have weather records.\n",
    "- Fill missing values using interpolation (numerical)  \n",
    "  and assign -1 for Wettercode to indicate â€œunknown weatherâ€.\n",
    "\n",
    "**Kiwo Event Flag:**\n",
    "- If missing â†’ fill with 0 (day outside event).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e16358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = merged_df.copy()\n",
    "\n",
    "# Remove rows where sales are missing\n",
    "clean_df = clean_df.dropna(subset=[\"Umsatz_umsatz\"])\n",
    "\n",
    "# Fill missing Wettercode with category -1\n",
    "if \"Wettercode_weather\" in clean_df.columns:\n",
    "    clean_df[\"Wettercode_weather\"] = clean_df[\"Wettercode_weather\"].fillna(-1)\n",
    "\n",
    "# Interpolate numerical weather values\n",
    "for col in clean_df.columns:\n",
    "    if (\"_weather\" in col) and (clean_df[col].dtype in [\"float64\", \"int64\"]):\n",
    "        clean_df[col] = clean_df[col].interpolate()\n",
    "\n",
    "clean_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"KielerWoche_kiwo\"] = clean_df[\"KielerWoche_kiwo\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fcdfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop wettercode_weather column\n",
    "df_no_weather_code = clean_df.drop(columns=[\"Wettercode_weather\"])\n",
    "df_no_weather_code.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76d324",
   "metadata": {},
   "source": [
    "## Adding extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_weather_code['Datum'] = pd.to_datetime(df_no_weather_code['Datum'], errors='coerce').dt.normalize()\n",
    "n_invalid = df_no_weather_code['Datum'].isna().sum()\n",
    "if n_invalid > 0:\n",
    "    print(f\"Warning: {n_invalid} rows have invalid 'Datum' and will have NaT in day columns. Sample:\")\n",
    "    display(df_no_weather_code[df_no_weather_code['Datum'].isna()].head())\n",
    "\n",
    "# Create integer and name columns for day of week\n",
    "df_no_weather_code['day_of_week'] = df_no_weather_code['Datum'].dt.weekday  # Monday=0 .. Sunday=6\n",
    "df_no_weather_code['day'] = df_no_weather_code['Datum'].dt.day_name()\n",
    "\n",
    "# Optional: make 'day' categorical ordered Monday..Sunday\n",
    "ordered_days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_no_weather_code['day'] = pd.Categorical(df_no_weather_code['day'], categories=ordered_days, ordered=True)\n",
    "\n",
    "print(\"\\nCounts per weekday:\")\n",
    "print(df_no_weather_code['day'].value_counts().sort_index())\n",
    "df_no_weather_code.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c754c",
   "metadata": {},
   "source": [
    "### Adding more weather data from meteo archive api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89809343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, date, timedelta\n",
    "import time\n",
    "from typing import Union, Iterable, Dict, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Simple in-memory cache for API responses\n",
    "try:\n",
    "    _OPEN_METEO_CACHE\n",
    "except NameError:\n",
    "    _OPEN_METEO_CACHE: Dict[str, Dict[str, float]] = {}\n",
    "\n",
    "def _iso_date(d: Union[str, date, datetime]) -> str:\n",
    "    if isinstance(d, str):\n",
    "        return datetime.fromisoformat(d).date().isoformat()\n",
    "    if isinstance(d, datetime):\n",
    "        return d.date().isoformat()\n",
    "    return d.isoformat()\n",
    "\n",
    "def fetch_open_meteo_daily_range(\n",
    "    start_date: Union[str, date, datetime],\n",
    "    end_date: Union[str, date, datetime],\n",
    "    latitude: float = 54.3233,\n",
    "    longitude: float = 10.1228,\n",
    "    timezone: str = \"Europe/Berlin\",\n",
    "    daily_vars: List[str] = None,\n",
    "    max_retries: int = 3,\n",
    "    timeout: float = 15.0,\n",
    "    use_cache: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch daily historical variables from Open-Meteo archive API for a given inclusive date range.\n",
    "    By default uses Kiel coordinates (lat=54.3233, lon=10.1228).\n",
    "    daily_vars example: ['precipitation_hours', 'sunshine_duration', 'rain_sum']\n",
    "    Returns a pandas.DataFrame indexed by date (datetime.date) with columns named after daily_vars.\n",
    "    \"\"\"\n",
    "    if daily_vars is None:\n",
    "        daily_vars = ['precipitation_hours', 'sunshine_duration', 'rain_sum']\n",
    "\n",
    "    start_iso = _iso_date(start_date)\n",
    "    end_iso = _iso_date(end_date)\n",
    "    start_dt = datetime.fromisoformat(start_iso).date()\n",
    "    end_dt = datetime.fromisoformat(end_iso).date()\n",
    "    if end_dt < start_dt:\n",
    "        raise ValueError(\"end_date must be >= start_date\")\n",
    "\n",
    "    # Build cache key based on parameters and requested range\n",
    "    key = f\"{latitude:.6f}_{longitude:.6f}_{start_iso}_{end_iso}_{','.join(daily_vars)}_{timezone}\"\n",
    "    if use_cache and key in _OPEN_METEO_CACHE:\n",
    "        df = _OPEN_METEO_CACHE[key].copy()\n",
    "        return df\n",
    "\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_iso,\n",
    "        \"end_date\": end_iso,\n",
    "        \"daily\": \",\".join(daily_vars),\n",
    "        \"timezone\": timezone\n",
    "    }\n",
    "\n",
    "    last_exc = None\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            daily = data.get(\"daily\", {})\n",
    "            times = daily.get(\"time\", []) or []\n",
    "            if not times:\n",
    "                raise ValueError(\"No 'time' in API response daily block.\")\n",
    "            results = {\"date\": pd.to_datetime(times).date}\n",
    "            for v in daily_vars:\n",
    "                vals = daily.get(v, None)\n",
    "                if vals is None:\n",
    "                    # If a requested variable is missing, fill with NaN\n",
    "                    results[v] = [np.nan] * len(times)\n",
    "                else:\n",
    "                    # Convert to numeric (floats), keep NaN if parsing fails\n",
    "                    results[v] = [float(x) if x is not None else np.nan for x in vals]\n",
    "            df = pd.DataFrame(results, index=pd.to_datetime(times).date)\n",
    "            df.index.name = \"date\"\n",
    "            df = df[[v for v in daily_vars]]  # ensure column order\n",
    "            if use_cache:\n",
    "                _OPEN_METEO_CACHE[key] = df.copy()\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(0.5 * attempt)\n",
    "                continue\n",
    "            raise RuntimeError(f\"Failed fetching Open-Meteo archive: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = df_no_weather_code['Datum'].min()\n",
    "max_date = df_no_weather_code['Datum'].max()\n",
    "##min_date = \"2013-07-01\"\n",
    "##max_date = \"2014-07-30\"\n",
    "print (f\"Fetching weather data from {min_date} to {max_date}...\")\n",
    "lat_kiel, lon_kiel = 54.3233, 10.1228\n",
    "daily_vars = ['precipitation_hours', 'sunshine_duration', 'rain_sum','temperature_2m_mean']\n",
    "df_extra_weather = fetch_open_meteo_daily_range(min_date, max_date, latitude=lat_kiel, longitude=lon_kiel, daily_vars=daily_vars)\n",
    "# print columns of df_extended_weather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_weather['sunshine_hours'] = df_extra_weather['sunshine_duration'] / 3600.0\n",
    "#df_extended_weather.head()\n",
    "\n",
    "# print row count for df_extended_weather and df_no_weather_code\n",
    "print(f\"Extended Weather Data Rows: {len(df_extra_weather)}\")\n",
    "print(f\"Sales Data Rows: {len(df_no_weather_code)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac1577",
   "metadata": {},
   "source": [
    "### Extended Data Frames\n",
    "Its with extra weather data from open meteo api: df_merged_extended_weather\n",
    "with rolling 7 day average as well for umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cb25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra_weather.index = pd.to_datetime(df_extra_weather.index)\n",
    "df_merged_extended_weather = df_no_weather_code.merge(\n",
    "    df_extra_weather,\n",
    "    left_on='Datum',\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Merged DataFrame Rows: {len(df_merged_extended_weather)}\")\n",
    "print(f\"Merged DataFrame Shape: {df_merged_extended_weather.shape}\")\n",
    "\n",
    "df_merged_extended_weather = df_merged_extended_weather.dropna(subset=[\"id_umsatz\"])\n",
    "df_merged_extended_weather[\"id_umsatz\"] = df_merged_extended_weather[\"id_umsatz\"].astype(\"int64\")\n",
    "df_merged_extended_weather = df_merged_extended_weather.dropna(subset=[\"KielerWoche_kiwo\"])\n",
    "df_merged_extended_weather[\"KielerWoche_kiwo\"] = df_merged_extended_weather[\"KielerWoche_kiwo\"].astype(\"int64\")\n",
    "\n",
    "df_merged_extended_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5677e",
   "metadata": {},
   "source": [
    "# Testing Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>Temperatur_weather</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>temperature_median</th>\n",
       "      <th>temperature_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>2014-03-10</td>\n",
       "      <td>15.8000</td>\n",
       "      <td>9.6</td>\n",
       "      <td>12.70000</td>\n",
       "      <td>12.70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>2018-04-22</td>\n",
       "      <td>14.5250</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.76250</td>\n",
       "      <td>12.76250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>2013-09-07</td>\n",
       "      <td>22.7625</td>\n",
       "      <td>19.7</td>\n",
       "      <td>21.23125</td>\n",
       "      <td>21.23125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>2017-08-26</td>\n",
       "      <td>19.5250</td>\n",
       "      <td>16.1</td>\n",
       "      <td>17.81250</td>\n",
       "      <td>17.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7418</th>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>11.8750</td>\n",
       "      <td>9.2</td>\n",
       "      <td>10.53750</td>\n",
       "      <td>10.53750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>10.6625</td>\n",
       "      <td>9.7</td>\n",
       "      <td>10.18125</td>\n",
       "      <td>10.18125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>2014-11-02</td>\n",
       "      <td>15.3750</td>\n",
       "      <td>12.9</td>\n",
       "      <td>14.13750</td>\n",
       "      <td>14.13750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>2.7750</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.78750</td>\n",
       "      <td>2.78750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5444</th>\n",
       "      <td>2016-03-23</td>\n",
       "      <td>9.1250</td>\n",
       "      <td>5.7</td>\n",
       "      <td>7.41250</td>\n",
       "      <td>7.41250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>2015-06-17</td>\n",
       "      <td>16.5000</td>\n",
       "      <td>12.9</td>\n",
       "      <td>14.70000</td>\n",
       "      <td>14.70000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Datum  Temperatur_weather  temperature_2m_mean  temperature_median  \\\n",
       "1690 2014-03-10             15.8000                  9.6            12.70000   \n",
       "9273 2018-04-22             14.5250                 11.0            12.76250   \n",
       "729  2013-09-07             22.7625                 19.7            21.23125   \n",
       "8046 2017-08-26             19.5250                 16.1            17.81250   \n",
       "7418 2017-04-21             11.8750                  9.2            10.53750   \n",
       "917  2013-10-15             10.6625                  9.7            10.18125   \n",
       "2849 2014-11-02             15.3750                 12.9            14.13750   \n",
       "8516 2017-11-25              2.7750                  2.8             2.78750   \n",
       "5444 2016-03-23              9.1250                  5.7             7.41250   \n",
       "4005 2015-06-17             16.5000                 12.9            14.70000   \n",
       "\n",
       "      temperature_mean  \n",
       "1690          12.70000  \n",
       "9273          12.76250  \n",
       "729           21.23125  \n",
       "8046          17.81250  \n",
       "7418          10.53750  \n",
       "917           10.18125  \n",
       "2849          14.13750  \n",
       "8516           2.78750  \n",
       "5444           7.41250  \n",
       "4005          14.70000  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_merged_extended_weather.tail()\n",
    "\n",
    "#show only columns Datum,temperature_weather, temperature_2m_mean\n",
    "df_merged_extended_weather[['Datum', 'Temperatur_weather', 'temperature_2m_mean']].sample(10)\n",
    "\n",
    "#calculate sample mean between temperature_weather and temperature_2m_mean\n",
    "#df_merged_extended_weather['temperature_mean'] = df_merged_extended_weather[['Temperatur_weather', 'temperature_2m_mean']].mean(axis=1)\n",
    "#df_merged_extended_weather['temperature_median'] = df_merged_extended_weather[['Temperatur_weather', 'temperature_2m_mean']].median(axis=1)\n",
    "#df_merged_extended_weather[['Datum', 'Temperatur_weather', 'temperature_2m_mean','temperature_median','temperature_mean']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce667e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "# Testing Regression\n",
    "reg_cols=['Umsatz_umsatz','umsatz_rolling7','KielerWoche_kiwo','Bewoelkung_weather', 'rain_sum', 'sunshine_hours','Temperatur_weather','Windgeschwindigkeit_weather','precipitation_hours','day_of_week']\n",
    "sns.pairplot(df_merged_extended_weather[reg_cols].dropna())\n",
    "df_merged_extended_weather[reg_cols].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ecbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Example regression formula (adjust as needed)\n",
    "formula = \"Umsatz_umsatz ~  Temperatur_weather  + C(Warengruppe_umsatz)\"\n",
    "\n",
    "# Drop rows with missing values in the regression columns\n",
    "regression_df = df_merged_extended_weather.dropna(subset=[\n",
    "    \"Umsatz_umsatz\", \"Bewoelkung_weather\", \"rain_sum\", \"sunshine_hours\",\n",
    "    \"Temperatur_weather\", \"Windgeschwindigkeit_weather\", \"precipitation_hours\", \"day_of_week\",\"Warengruppe_umsatz\"\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model = smf.ols(formula, data=regression_df).fit()\n",
    "\n",
    "# Show summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991529d",
   "metadata": {},
   "source": [
    "# Merging test data from test csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test.csv into testdf\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Quick checks\n",
    "print(\"Shape:\", df_test.shape)\n",
    "display(df_test.head())\n",
    "display(df_test.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b285d8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1830, 4)\n",
      "Missing Temperatur_weather: 65\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Datum</th>\n",
       "      <th>Warengruppe</th>\n",
       "      <th>Temperatur_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1808011</td>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>1</td>\n",
       "      <td>23.7625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1808021</td>\n",
       "      <td>2018-08-02</td>\n",
       "      <td>1</td>\n",
       "      <td>26.1875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1808031</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>1</td>\n",
       "      <td>27.6625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1808041</td>\n",
       "      <td>2018-08-04</td>\n",
       "      <td>1</td>\n",
       "      <td>25.1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1808051</td>\n",
       "      <td>2018-08-05</td>\n",
       "      <td>1</td>\n",
       "      <td>21.3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      Datum  Warengruppe  Temperatur_weather\n",
       "0  1808011 2018-08-01            1             23.7625\n",
       "1  1808021 2018-08-02            1             26.1875\n",
       "2  1808031 2018-08-03            1             27.6625\n",
       "3  1808041 2018-08-04            1             25.1375\n",
       "4  1808051 2018-08-05            1             21.3000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Datum</th>\n",
       "      <th>Warengruppe</th>\n",
       "      <th>Temperatur_weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>1812226</td>\n",
       "      <td>2018-12-22</td>\n",
       "      <td>6</td>\n",
       "      <td>4.3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>1812236</td>\n",
       "      <td>2018-12-23</td>\n",
       "      <td>6</td>\n",
       "      <td>6.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>1812246</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>6</td>\n",
       "      <td>2.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>1812276</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>6</td>\n",
       "      <td>7.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>1812286</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>6</td>\n",
       "      <td>7.3125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id      Datum  Warengruppe  Temperatur_weather\n",
       "1825  1812226 2018-12-22            6              4.3000\n",
       "1826  1812236 2018-12-23            6              6.4500\n",
       "1827  1812246 2018-12-24            6              2.5000\n",
       "1828  1812276 2018-12-27            6              7.1250\n",
       "1829  1812286 2018-12-28            6              7.3125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## add weather Temperatur_weather from df_weather\n",
    "df_test['Datum'] = pd.to_datetime(df_test['Datum'], errors='coerce').dt.normalize()\n",
    "df_weather['Datum'] = pd.to_datetime(df_weather['Datum'], errors='coerce').dt.normalize()\n",
    "\n",
    "\n",
    "# Left-join temperature from df_weather onto the test DataFrame (keep all test rows)\n",
    "df_test_weather = df_test.merge(\n",
    "    df_weather[['Datum', 'Temperatur_weather']],\n",
    "    on='Datum',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Quick sanity checks\n",
    "print(\"Shape:\", df_test_weather.shape)\n",
    "print(\"Missing Temperatur_weather:\", df_test_weather['Temperatur_weather'].isna().sum())\n",
    "display(df_test_weather.head())\n",
    "display(df_test_weather.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba3f73",
   "metadata": {},
   "source": [
    "## Predicting with df_test_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting with df_test_weather\n",
    "# rename df_test_weather.Warengruppe to Warengruppe_umsatz for consistency\n",
    "df_test_weather = df_test_weather.rename(columns={\"Warengruppe\": \"Warengruppe_umsatz\"})\n",
    "\n",
    "# Required columns for this model\n",
    "req_cols = ['Temperatur_weather', 'Warengruppe_umsatz']\n",
    "\n",
    "# Sanity checks\n",
    "missing = [c for c in req_cols if c not in df_test_weather.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns for prediction: {missing}\")\n",
    "\n",
    "# Ensure numeric temperature\n",
    "df_test_weather['Temperatur_weather'] = pd.to_numeric(df_test_weather['Temperatur_weather'], errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows missing predictor values\n",
    "#df_predicted = df_test_weather.dropna(subset=req_cols).copy()\n",
    "df_predicted = df_test_weather.copy()\n",
    "\n",
    "\n",
    "\n",
    "# Predict in next cell\n",
    "#df_predicted['predicted_Umsatz_umsatz'] = model.predict(df_predicted)\n",
    "\n",
    "print(df_predicted.shape)\n",
    "display(df_predicted.head())\n",
    "display(df_predicted.tail())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b17635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#predicting with df_test_weather\n",
    "df_predicted['predicted_Umsatz_umsatz'] = model.predict(df_predicted)\n",
    "\n",
    "print(df_predicted.shape)\n",
    "display(df_predicted.head())\n",
    "display(df_predicted.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2236d1",
   "metadata": {},
   "source": [
    "## generating data submission csv from df_predicted\n",
    "Will write a csv file in data named predicted.csv with these columns id,umsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a265a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output frame and clean missing rows\n",
    "df_out_predicted = df_predicted[[\"id\", \"predicted_Umsatz_umsatz\"]].copy()\n",
    "#rename columns accoding to kaggle submission requirements\n",
    "df_out_predicted.rename(columns={ \"predicted_Umsatz_umsatz\": 'umsatz'}, inplace=True)\n",
    "\n",
    "#print columns name df_out_predicted\n",
    "print(df_out_predicted.columns)\n",
    "print(df_out_predicted.shape)\n",
    "\n",
    "# write to csv\n",
    "df_out_predicted.to_csv(\"data/predicted.csv\", index=False)\n",
    "print(\"Wrote data/predicted.csv with\", len(df_out_predicted), \"rows.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
